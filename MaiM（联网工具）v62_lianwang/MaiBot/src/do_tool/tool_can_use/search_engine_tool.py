from src.do_tool.tool_can_use.base_tool import BaseTool
from src.common.logger import get_module_logger
from typing import Dict, Any, List, Optional, Union, Tuple
import asyncio
import time
import os
from datetime import datetime
import re
from src.common.utils import log_async_performance, PerformanceTimer

logger = get_module_logger("search_engine_tool")


class SearchEngineTool(BaseTool):
    """é›†æˆå¼æœç´¢å¼•æ“å·¥å…·ï¼Œå¯æ™ºèƒ½åˆ¤æ–­æ˜¯å¦ä½¿ç”¨çŸ¥è¯†åº“æˆ–æ‰§è¡Œç½‘ç»œæœç´¢"""

    name = "search_engine"
    description = "æ™ºèƒ½æœç´¢å¼•æ“ï¼Œä¼šå…ˆåœ¨çŸ¥è¯†åº“ä¸­æŸ¥æ‰¾ä¿¡æ¯ï¼Œå¦‚æœéœ€è¦å†æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œè‡ªåŠ¨å­˜å‚¨æœ‰ä»·å€¼çš„ä¿¡æ¯"
    parameters = {
        "type": "object",
        "properties": {
            "query": {"type": "string", "description": "è¦æœç´¢çš„æŸ¥è¯¢å†…å®¹"},
            "force_web_search": {"type": "boolean", "description": "æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œç½‘ç»œæœç´¢ï¼Œé»˜è®¤ä¸ºfalse"},
            "time_range": {"type": "string", "description": "æœç´¢ç»“æœçš„æ—¶é—´èŒƒå›´ï¼Œå¯é€‰å€¼: day, week, month, yearï¼Œé»˜è®¤ä¸ºmonth"},
            "num_results": {"type": "integer", "description": "è¿”å›çš„æœç´¢ç»“æœæ•°é‡ï¼Œé»˜è®¤ä¸º5"},
            "chat_id": {"type": "string", "description": "èŠå¤©IDï¼Œç”¨äºæœç´¢å†·å´æ§åˆ¶"},
            "tags": {"type": "array", "items": {"type": "string"}, "description": "æœç´¢çŸ¥è¯†åº“æ—¶çš„æ ‡ç­¾è¿‡æ»¤"},
            "min_similarity": {"type": "number", "description": "çŸ¥è¯†åº“æœç´¢çš„æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.4"},
            "prioritize_recent": {"type": "boolean", "description": "æ˜¯å¦ä¼˜å…ˆè¿”å›è¾ƒæ–°çš„çŸ¥è¯†ï¼Œé»˜è®¤ä¸ºtrue"},
        },
        "required": ["query"],
    }

    def __init__(self):
        """åˆå§‹åŒ–æ™ºèƒ½æœç´¢å¼•æ“å·¥å…·"""
        # å†·å´æ—¶é—´è®°å½•
        self._last_search_times = {}
        # é»˜è®¤å†·å´æ—¶é—´(ç§’)
        self.default_cooldown = 1800  # 30åˆ†é’Ÿ
        # ç»“æœç¼“å­˜
        self._cache = {}
        # ç¼“å­˜æœ‰æ•ˆæœŸ(ç§’)
        self.cache_ttl = 3600  # 1å°æ—¶

    async def execute(self, function_args: Dict[str, Any], message_txt: str = "") -> Dict[str, Any]:
        """æ‰§è¡Œæ™ºèƒ½æœç´¢
        
        Args:
            function_args: å·¥å…·å‚æ•°
            message_txt: åŸå§‹æ¶ˆæ¯æ–‡æœ¬
            
        Returns:
            Dict: å·¥å…·æ‰§è¡Œç»“æœ
        """
        try:
            with PerformanceTimer("search_engine_execute") as timer:
                # è·å–å‚æ•°
                query = function_args.get("query")
                force_web_search = function_args.get("force_web_search", False)
                time_range = function_args.get("time_range", "month")
                num_results = function_args.get("num_results", 5)
                chat_id = function_args.get("chat_id", "")
                tags = function_args.get("tags", [])
                min_similarity = function_args.get("min_similarity", 0.4)
                prioritize_recent = function_args.get("prioritize_recent", True)
                
                # è·å–æœç´¢ç»“æœå›å¤æ¨¡å¼
                from src.plugins.config.config import global_config
                search_result_mode = getattr(global_config, "search_result_mode", "personalized")
                store_search_results = getattr(global_config, "store_search_results", True)
                
                # è¿‡æ»¤æŸ¥è¯¢ä¸­çš„æœºå™¨äººåç§°å’Œåˆ«å
                bot_nickname = getattr(global_config, "BOT_NICKNAME", None)
                bot_alias_names = getattr(global_config, "BOT_ALIAS_NAMES", [])
                
                original_query = query
                # æ„å»ºè¦è¿‡æ»¤çš„åç§°åˆ—è¡¨
                names_to_filter = [name for name in ([bot_nickname] + bot_alias_names) if name]
                # æŒ‰é•¿åº¦é™åºæ’åºï¼Œä»¥ä¾¿å…ˆåŒ¹é…è¾ƒé•¿çš„åç§°
                names_to_filter.sort(key=len, reverse=True)
                
                # è¿‡æ»¤æ‰æŸ¥è¯¢ä¸­çš„æœºå™¨äººåå­—å’Œåˆ«å
                for name in names_to_filter:
                    # å°è¯•ç§»é™¤åç§°åè·Ÿé€—å·æˆ–ç©ºæ ¼çš„æƒ…å†µ
                    query = re.sub(rf'{re.escape(name)}[,ï¼Œ\s]+', '', query, flags=re.IGNORECASE)
                    # å°è¯•ç§»é™¤å•ç‹¬çš„åç§°
                    query = re.sub(rf'^{re.escape(name)}$', '', query, flags=re.IGNORECASE)
                    
                # å»é™¤å¯èƒ½ç•™ä¸‹çš„å‰åç©ºæ ¼
                query = query.strip()
                # å¦‚æœè¿‡æ»¤åæŸ¥è¯¢ä¸ºç©ºï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ä½†ç§»é™¤ç§°å‘¼éƒ¨åˆ†
                if not query:
                    # å°è¯•ä»…ä¿ç•™é—®é¢˜éƒ¨åˆ†
                    for name in names_to_filter:
                        if original_query.lower().startswith(name.lower()):
                            query = original_query[len(name):].strip(' ,.ï¼Œã€‚ã€?ï¼Ÿ!ï¼')
                            break
                    # å¦‚æœè¿˜æ˜¯ç©ºï¼Œåˆ™ä½¿ç”¨åŸå§‹æŸ¥è¯¢
                    if not query:
                        query = original_query
                
                # è®°å½•è¿‡æ»¤å‰åçš„å˜åŒ–
                if query != original_query:
                    logger.info(f"è¿‡æ»¤æœºå™¨äººåç§°: {original_query} -> {query}")
                
                # æ£€æŸ¥ç¼“å­˜
                timer.start_section("check_cache")
                cache_key = f"{query}:{time_range}:{num_results}:{'-'.join(tags)}"
                cache_result = self._check_cache(cache_key)
                if cache_result:
                    logger.info(f"ä½¿ç”¨ç¼“å­˜ç»“æœï¼ŒæŸ¥è¯¢: {query}")
                    return {
                        "name": self.name, 
                        "content": cache_result.get("content", ""),
                        "from_cache": True,
                        "source": cache_result.get("source", "cache"),
                        "results": cache_result.get("results", [])
                    }
                timer.end_section()
                
                # æ£€æŸ¥å†·å´æ—¶é—´ï¼ˆé™¤éå¼ºåˆ¶æœç´¢ï¼‰
                if not force_web_search and chat_id:
                    timer.start_section("check_cooldown")
                    cooldown_result = self._check_cooldown(chat_id)
                    if not cooldown_result[0]:  # å†·å´ä¸­
                        logger.info(f"æœç´¢å†·å´ä¸­ï¼Œå‰©ä½™æ—¶é—´:{cooldown_result[1]:.1f}ç§’")
                        return {
                            "name": self.name, 
                            "content": f"æœç´¢å†·å´ä¸­ï¼Œå‰©ä½™{cooldown_result[1]:.1f}ç§’", 
                            "skipped": True, 
                            "reason": "å†·å´ä¸­"
                        }
                    timer.end_section()
                
                # 1. å…ˆä»çŸ¥è¯†åº“æœç´¢
                timer.start_section("search_knowledge")
                knowledge_results = []
                from src.plugins.config.config import global_config
                knowledge_base_enable = getattr(global_config, "knowledge_base_enable", True)
                
                if knowledge_base_enable:
                    knowledge_results = await self._search_knowledge(
                        query=query, 
                        tags=tags, 
                        limit=num_results,
                        min_similarity=min_similarity,
                        prioritize_recent=prioritize_recent
                    )
                else:
                    logger.info("çŸ¥è¯†åº“æŸ¥è¯¢å·²ç¦ç”¨ï¼Œè·³è¿‡çŸ¥è¯†åº“æœç´¢")
                timer.end_section()
                
                # 2. åˆ†æçŸ¥è¯†åº“ç»“æœè´¨é‡å¹¶å†³å®šæ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢
                timer.start_section("decide_web_search")
                need_web_search, reason = await self._need_web_search(
                    query=query,
                    knowledge_results=knowledge_results,
                    force_web_search=force_web_search
                )
                timer.end_section()
                
                # 3. å¦‚æœéœ€è¦ï¼Œæ‰§è¡Œç½‘ç»œæœç´¢
                web_results = []
                if need_web_search:
                    timer.start_section("web_search")
                    # æ›´æ–°æœ€åæœç´¢æ—¶é—´
                    if chat_id:
                        self._last_search_times[chat_id] = asyncio.get_event_loop().time()
                    
                    # è·å–web_searchå·¥å…·å®ä¾‹å¹¶æ‰§è¡Œæœç´¢
                    from src.do_tool.tool_can_use import get_tool_instance
                    web_search_tool = get_tool_instance("web_search")
                    
                    if web_search_tool:
                        web_search_result = await web_search_tool.execute({
                            "query": query,
                            "num_results": num_results,
                            "time_range": time_range,
                            "force_search": True  # å¼ºåˆ¶æœç´¢ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åšäº†å†³ç­–
                        })
                        
                        # åœ¨ä¸ªæ€§åŒ–æ¨¡å¼ä¸‹ï¼Œå°†ç½‘ç»œæœç´¢ç»“æœå­˜å‚¨åˆ°çŸ¥è¯†åº“
                        if search_result_mode == "personalized" and store_search_results:
                            if web_search_result and "content" in web_search_result and not web_search_result.get("skipped", False):
                                timer.start_section("store_knowledge")
                                try:
                                    store_knowledge_tool = get_tool_instance("store_knowledge")
                                    if store_knowledge_tool:
                                        await store_knowledge_tool.execute({
                                            "query": query,
                                            "content": web_search_result["content"],
                                            "source": "web_search",
                                            "importance": 3,  # é»˜è®¤ä¸­ç­‰é‡è¦æ€§
                                            "tags": ["æœç´¢ç»“æœ", "è‡ªåŠ¨å­˜å‚¨", f"æœç´¢æ—¶é—´_{datetime.now().strftime('%Y%m%d')}"]
                                        })
                                        logger.info("æœç´¢ç»“æœå·²ä¿å­˜åˆ°çŸ¥è¯†åº“")
                                except Exception as e:
                                    logger.error(f"å­˜å‚¨æœç´¢ç»“æœå¤±è´¥: {e}")
                                timer.end_section()
                        
                        web_results = web_search_result
                    else:
                        logger.error("æ— æ³•è·å–web_searchå·¥å…·å®ä¾‹")
                    timer.end_section()
                
                # 4. æ•´åˆç»“æœï¼Œæ ¹æ®ç»“æœæ¨¡å¼å†³å®šæ ¼å¼
                timer.start_section("combine_results")
                combined_results, content = self._combine_results(
                    query=query,
                    knowledge_results=knowledge_results,
                    web_results=web_results,
                    need_web_search=need_web_search,
                    result_mode=search_result_mode
                )
                timer.end_section()
                
                # 5. ç¼“å­˜ç»“æœ
                timer.start_section("cache_results")
                self._cache_results(cache_key, content, combined_results, "combined" if need_web_search else "knowledge")
                # æ¸…ç†è¿‡æœŸç¼“å­˜
                self._clean_cache()
                timer.end_section()
                
                # æ ¹æ®æœç´¢ç»“æœæ¨¡å¼è¿”å›ä¸åŒç»“æ„
                if search_result_mode == "direct":
                    # ç›´æ¥æ¨¡å¼ï¼šä½¿ç”¨è¡¨æ ¼å¼ç»“æ„åŒ–è¾“å‡º
                    return {
                        "name": self.name,
                        "content": content,
                        "results": combined_results,
                        "source": "combined" if need_web_search else "knowledge",
                        "web_search_used": need_web_search,
                        "web_search_reason": reason,
                        "result_mode": "direct"
                    }
                else:
                    # ä¸ªæ€§åŒ–æ¨¡å¼ï¼šè¿”å›é€‚åˆèå…¥å¯¹è¯ä¸­çš„ç»“æœ
                    # ç¡®ä¿å†…å®¹ä¸­ä¸åŒ…å«æ³¨é‡Šæˆ–å…ƒæ•°æ®æ ‡è®°
                    final_content = content
                    # å»é™¤å¯èƒ½çš„æ³¨é‡Šå’Œå…ƒæ•°æ®
                    # åŒ¹é…ä»»ä½•æ‹¬å·å†…çš„æ³¨é‡Šå†…å®¹
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?[æ³¨è¨»é‡Šé‡‹][:ï¼š].*?[\)ï¼‰]', '', final_content)
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?æ³¨æ„.*?[\)ï¼‰]', '', final_content)
                    # åŒ¹é…æ‹¬å·å†…å¸¦å…³é”®è¯çš„å†…å®¹
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?æ•´åˆ.*?[\)ï¼‰]', '', final_content)
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?è½¬åŒ–.*?[\)ï¼‰]', '', final_content)
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?æ‘˜è‡ª.*?[\)ï¼‰]', '', final_content)
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?ç»“åˆ.*?[\)ï¼‰]', '', final_content)
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?çŸ¥è¯†åº“.*?[\)ï¼‰]', '', final_content)
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?æ¥æº.*?[\)ï¼‰]', '', final_content)
                    final_content = re.sub(r'\s*[\(ï¼ˆ].*?ä¿¡æ¯.*?[\)ï¼‰]', '', final_content)
                    # å»é™¤å†—ä½™çš„ç©ºç™½å’Œæ–­è¡Œ
                    final_content = re.sub(r'\n{3,}', '\n\n', final_content)
                    final_content = re.sub(r'\s+$', '', final_content)
                    
                    return {
                        "name": self.name,
                        "content": final_content,
                        "results": combined_results,
                        "source": "combined" if need_web_search else "knowledge",
                        "web_search_used": need_web_search,
                        "web_search_reason": reason,
                        "result_mode": "personalized"
                    }
                
        except Exception as e:
            logger.error(f"æ™ºèƒ½æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {"name": self.name, "content": f"æœç´¢å¤±è´¥: {str(e)}"}
    
    async def _search_knowledge(self, query: str, tags: List[str] = None, limit: int = 5, 
                               min_similarity: float = 0.4, prioritize_recent: bool = True) -> List[Dict]:
        """ä»çŸ¥è¯†åº“æœç´¢ä¿¡æ¯
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            tags: æ ‡ç­¾è¿‡æ»¤
            limit: æœ€å¤§è¿”å›ç»“æœæ•°
            min_similarity: æœ€å°ç›¸ä¼¼åº¦é˜ˆå€¼
            prioritize_recent: æ˜¯å¦ä¼˜å…ˆè¿”å›è¾ƒæ–°çš„å†…å®¹
            
        Returns:
            List[Dict]: æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            from src.do_tool.tool_can_use import get_tool_instance
            search_knowledge_tool = get_tool_instance("search_knowledge")
            
            if search_knowledge_tool:
                results = await search_knowledge_tool.execute({
                    "query": query,
                    "min_similarity": min_similarity,
                    "prioritize_recent": prioritize_recent,
                    "limit": limit
                })
                
                # å¤„ç†ç»“æœ
                if results and isinstance(results, dict) and "results" in results:
                    return results.get("results", [])
                    
            # å°è¯•ä½¿ç”¨store_knowledgeå·¥å…·çš„search_knowledgeæ–¹æ³•
            store_knowledge_tool = get_tool_instance("store_knowledge")
            if store_knowledge_tool and hasattr(store_knowledge_tool, "search_knowledge"):
                results = await store_knowledge_tool.search_knowledge(
                    query=query,
                    tags=tags,
                    limit=limit,
                    prioritize_recent=prioritize_recent,
                    min_similarity=min_similarity
                )
                return results
                
            logger.warning("æ— æ³•è·å–çŸ¥è¯†æœç´¢å·¥å…·å®ä¾‹")
            return []
            
        except Exception as e:
            logger.error(f"çŸ¥è¯†åº“æœç´¢å¤±è´¥: {str(e)}")
            return []
    
    async def _need_web_search(self, query: str, knowledge_results: List[Dict], force_web_search: bool = False) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰§è¡Œç½‘ç»œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            knowledge_results: çŸ¥è¯†åº“æœç´¢ç»“æœ
            force_web_search: æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œç½‘ç»œæœç´¢
            
        Returns:
            Tuple[bool, str]: (æ˜¯å¦éœ€è¦ç½‘ç»œæœç´¢, åŸå› )
        """
        from src.plugins.config.config import global_config
        knowledge_base_enable = getattr(global_config, "knowledge_base_enable", True)
        
        if force_web_search:
            return True, "ç”¨æˆ·å¼ºåˆ¶è¦æ±‚æœç´¢"
        
        # å¦‚æœçŸ¥è¯†åº“è¢«ç¦ç”¨ï¼Œç›´æ¥è¿›è¡Œç½‘ç»œæœç´¢
        if not knowledge_base_enable:
            return True, "çŸ¥è¯†åº“æŸ¥è¯¢å·²ç¦ç”¨"
        
        # å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ç»“æœï¼Œåˆ™éœ€è¦ç½‘ç»œæœç´¢
        if not knowledge_results:
            return True, "çŸ¥è¯†åº“ä¸­æ— åŒ¹é…ç»“æœ"
        
        # åˆ†ææŸ¥è¯¢æ˜¯å¦åŒ…å«æ—¶é—´æ•æ„Ÿè¯
        time_patterns = [
            r"ä»Šå¤©", r"æ˜¨å¤©", r"å‰å¤©", r"æ˜å¤©", r"åå¤©", r"æœ€è¿‘", 
            r"è¿™å‘¨", r"ä¸Šå‘¨", r"ä¸‹å‘¨", r"è¿™ä¸ªæœˆ", r"ä¸Šä¸ªæœˆ", r"ä»Šå¹´",
            r"æœ€æ–°", r"åˆšåˆš", r"ç°åœ¨", r"ç°ä»Š", r"ç›®å‰", r"å½“å‰",
            r"å®æ—¶", r"æœ€æ–°åŠ¨æ€", r"æ–°é—»"
        ]
        
        if any(re.search(pattern, query) for pattern in time_patterns):
            # æ£€æŸ¥çŸ¥è¯†åº“ç»“æœçš„æ—¶é—´æˆ³
            current_time = time.time()
            newest_result_time = 0
            
            for result in knowledge_results:
                timestamp = result.get("timestamp", 0)
                if timestamp > newest_result_time:
                    newest_result_time = timestamp
            
            # å¦‚æœæœ€æ–°ç»“æœè¶…è¿‡3å¤©ï¼Œåˆ™è®¤ä¸ºéœ€è¦åˆ·æ–°
            if current_time - newest_result_time > 259200:  # 3å¤© = 259200ç§’
                return True, "çŸ¥è¯†åº“ä¿¡æ¯å¯èƒ½å·²è¿‡æ—¶"
        
        # æ£€æŸ¥æœç´¢è´¨é‡
        search_keywords = ["æœç´¢", "æŸ¥ä¸€ä¸‹", "æŸ¥ä¸€æŸ¥", "æŸ¥æ‰¾", "æŸ¥è¯¢", "æœä¸€ä¸‹", "æœä¸€æœ", 
                           "æœæœçœ‹", "æŸ¥æŸ¥çœ‹", "ç™¾åº¦", "è°·æ­Œ", "æœä¸€æœ", "æ‰¾æ‰¾çœ‹"]
        if any(keyword in query for keyword in search_keywords):
            return True, "ç”¨æˆ·æ˜ç¡®è¦æ±‚æœç´¢"
        
        # æ£€æŸ¥çŸ¥è¯†åº“ç»“æœè´¨é‡
        if knowledge_results:
            # å¦‚æœç»“æœç›¸ä¼¼åº¦å¤ªä½ï¼Œå¯èƒ½éœ€è¦ç½‘ç»œæœç´¢è¡¥å……
            if knowledge_results[0].get("similarity", 0) < 0.6:
                return True, "çŸ¥è¯†åº“åŒ¹é…åº¦ä¸é«˜ï¼Œéœ€è¦è¡¥å……ä¿¡æ¯"
                
            # å¦‚æœé—®é¢˜æ˜¯ä¸“ä¸šæ€§çš„ï¼Œæ£€æŸ¥çŸ¥è¯†åº“ç»“æœçš„æ¥æº
            professional_patterns = [
                r"æŠ€æœ¯", r"ç§‘æŠ€", r"å­¦æœ¯", r"ç ”ç©¶", r"è®ºæ–‡", r"ä¸“ä¸š", 
                r"é¢†åŸŸ", r"æ–¹æ³•", r"åŸç†", r"æœºåˆ¶", r"ç³»ç»Ÿ", r"æ¡†æ¶"
            ]
            
            if any(re.search(pattern, query) for pattern in professional_patterns):
                # æ£€æŸ¥æ˜¯å¦æœ‰æ¥è‡ªå¯é æ¥æºçš„çŸ¥è¯†
                reliable_sources = ["web_search", "education", "academic", "research"]
                has_reliable_source = False
                
                for result in knowledge_results:
                    if result.get("source") in reliable_sources:
                        has_reliable_source = True
                        break
                
                if not has_reliable_source:
                    return True, "ä¸“ä¸šé—®é¢˜éœ€è¦æ›´å¯é çš„ä¿¡æ¯æ¥æº"
        
        # é»˜è®¤æƒ…å†µä¸‹ï¼ŒçŸ¥è¯†åº“ç»“æœè¶³å¤Ÿå¥½ï¼Œä¸éœ€è¦ç½‘ç»œæœç´¢
        return False, "çŸ¥è¯†åº“ç»“æœå·²æ»¡è¶³éœ€æ±‚"
    
    def _combine_results(self, query: str, knowledge_results: List[Dict], web_results: Any, need_web_search: bool, result_mode: str = "personalized") -> Tuple[List[Dict], str]:
        """æ•´åˆçŸ¥è¯†åº“å’Œç½‘ç»œæœç´¢ç»“æœ
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            knowledge_results: çŸ¥è¯†åº“æœç´¢ç»“æœ
            web_results: ç½‘ç»œæœç´¢ç»“æœ
            need_web_search: æ˜¯å¦æ‰§è¡Œäº†ç½‘ç»œæœç´¢
            result_mode: ç»“æœæ¨¡å¼ï¼Œå¯é€‰å€¼ï¼špersonalizedï¼ˆæ‹ŸäººåŒ–ï¼‰ã€directï¼ˆç›´æ¥è¾“å‡ºï¼‰
            
        Returns:
            Tuple[List[Dict], str]: (åˆå¹¶åçš„ç»“æœåˆ—è¡¨, æ ¼å¼åŒ–çš„å†…å®¹)
        """
        combined_results = []
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M")
        from src.plugins.config.config import global_config
        
        # è·å–é…ç½®é¡¹
        direct_results_max_length = getattr(global_config, "direct_results_max_length", 1000)
        personalized_format_enabled = getattr(global_config, "personalized_format_enabled", True)
        use_structured_format = getattr(global_config, "use_structured_format", True)
        show_metadata = getattr(global_config, "show_metadata", True)
        max_results_per_source = getattr(global_config, "max_results_per_source", 5)
        
        # 1. æ·»åŠ çŸ¥è¯†åº“ç»“æœ
        if knowledge_results:
            for i, result in enumerate(knowledge_results):
                combined_results.append({
                    "source": "çŸ¥è¯†åº“",
                    "title": f"çŸ¥è¯† {i+1}",
                    "content": result.get("content", ""),
                    "url": "",
                    "similarity": result.get("similarity", 0),
                    "time": result.get("timestamp", 0),
                    "tags": result.get("tags", [])
                })
        
        # 2. å¦‚æœæ‰§è¡Œäº†ç½‘ç»œæœç´¢ï¼Œæ·»åŠ ç½‘ç»œç»“æœ
        if need_web_search and web_results and isinstance(web_results, dict) and "content" in web_results:
            web_content = web_results.get("content", "")
            
            # å°è¯•ä»æ ¼å¼åŒ–çš„å†…å®¹ä¸­æå–ç»“æœ
            web_items = []
            try:
                if "æœç´¢ç»“æœ" in web_content:
                    # åˆ†æweb_contentå¹¶æå–ç»“æ„åŒ–ä¿¡æ¯
                    result_blocks = re.split(r'ğŸ“Œ ç»“æœ \d+:', web_content)
                    
                    for block in result_blocks[1:]:  # è·³è¿‡ç¬¬ä¸€ä¸ªå¯èƒ½æ˜¯æ ‡é¢˜éƒ¨åˆ†
                        title_match = re.search(r'ğŸ“ æ ‡é¢˜: (.*?)[\n\r]', block)
                        url_match = re.search(r'ğŸ”— é“¾æ¥: (.*?)[\n\r]', block)
                        date_match = re.search(r'ğŸ“… å‘å¸ƒæ—¥æœŸ: (.*?)[\n\r]', block)
                        content_parts = re.split(r'ğŸ“„ å†…å®¹:', block)
                        
                        if title_match and len(content_parts) > 1:
                            title = title_match.group(1).strip()
                            url = url_match.group(1).strip() if url_match else ""
                            date = date_match.group(1).strip() if date_match else ""
                            content = content_parts[1].strip()
                            
                            web_items.append({
                                "source": "ç½‘ç»œ",
                                "title": title,
                                "content": content,
                                "url": url,
                                "date": date
                            })
            except Exception as e:
                logger.error(f"è§£æç½‘ç»œæœç´¢ç»“æœå¤±è´¥: {e}")
            
            # å¦‚æœæˆåŠŸæå–äº†ç»“æ„åŒ–ä¿¡æ¯ï¼Œæ·»åŠ åˆ°åˆå¹¶ç»“æœ
            if web_items:
                combined_results.extend(web_items)
            else:
                # å¦åˆ™ï¼Œæ·»åŠ æ•´ä¸ªå†…å®¹ä½œä¸ºä¸€ä¸ªç»“æœ
                combined_results.append({
                    "source": "ç½‘ç»œ",
                    "title": "ç½‘ç»œæœç´¢ç»“æœ",
                    "content": web_content,
                    "url": "",
                })
        
        # 3. æ ¹æ®æ¨¡å¼æ ¼å¼åŒ–åˆå¹¶ç»“æœ
        if result_mode == "direct":
            # ç›´æ¥æ¨¡å¼ï¼šç»“æ„åŒ–è¾“å‡ºä¸ç»è¿‡å¤„ç†çš„æœç´¢ç»“æœ
            if use_structured_format:
                # ä½¿ç”¨ç»“æ„åŒ–æ ¼å¼è¾“å‡º
                formatted = f"ã€æœç´¢ç»“æœã€‘æŸ¥è¯¢: {query}\n\n"
                
                # çŸ¥è¯†åº“ç»“æœ
                knowledge_count = min(len([r for r in combined_results if r.get("source") == "çŸ¥è¯†åº“"]), max_results_per_source)
                knowledge_items = [r for r in combined_results if r.get("source") == "çŸ¥è¯†åº“"]
                
                if knowledge_items:
                    formatted += f"===== çŸ¥è¯†åº“ç»“æœ ({knowledge_count}/{len(knowledge_items)}æ¡) =====\n\n"
                    
                    for i, result in enumerate(knowledge_items[:max_results_per_source]):
                        formatted += f"[{i+1}] {'-'*40}\n"
                        formatted += f"å†…å®¹: {result.get('content', '')}\n"
                        
                        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦å±•ç¤ºå…ƒæ•°æ®
                        if show_metadata:
                            formatted += f"{'='*10} å…ƒæ•°æ® {'='*10}\n"
                            formatted += f"ç›¸ä¼¼åº¦: {result.get('similarity', 0):.2f}\n"
                            if result.get('tags'):
                                formatted += f"æ ‡ç­¾: {', '.join(result.get('tags', []))}\n"
                            if result.get('time'):
                                formatted += f"æ—¶é—´: {datetime.fromtimestamp(result.get('time', 0)).strftime('%Y-%m-%d')}\n"
                        formatted += f"{'-'*40}\n\n"
                
                # ç½‘ç»œæœç´¢ç»“æœ
                web_items = [r for r in combined_results if r.get("source") == "ç½‘ç»œ"]
                web_count = min(len(web_items), max_results_per_source)
                if web_items:
                    formatted += f"===== ç½‘ç»œæœç´¢ç»“æœ ({web_count}/{len(web_items)}æ¡) =====\n\n"
                    
                    for i, result in enumerate(web_items[:max_results_per_source]):
                        formatted += f"[{i+1}] {'-'*40}\n"
                        
                        # æ ‡é¢˜å’Œé“¾æ¥
                        if result.get('title'):
                            formatted += f"æ ‡é¢˜: {result.get('title', '')}\n"
                        if result.get('url'):
                            formatted += f"é“¾æ¥: {result.get('url', '')}\n"
                            
                        # æ—¥æœŸä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰    
                        if result.get('date') and show_metadata:
                            formatted += f"æ—¥æœŸ: {result.get('date', '')}\n"
                            
                        # å†…å®¹
                        formatted += f"å†…å®¹: {result.get('content', '')}\n"
                        
                        formatted += f"{'-'*40}\n\n"
                
                # å¦‚æœè¿˜æœ‰æ›´å¤šç»“æœ
                if len(knowledge_items) > max_results_per_source or len(web_items) > max_results_per_source:
                    formatted += "...\n"
                    total_more = (len(knowledge_items) - max_results_per_source if len(knowledge_items) > max_results_per_source else 0) + \
                                 (len(web_items) - max_results_per_source if len(web_items) > max_results_per_source else 0)
                    formatted += f"è¿˜æœ‰ {total_more} æ¡ç»“æœæœªæ˜¾ç¤ºã€‚\n\n"
            else:
                # ä½¿ç”¨ç®€å•æ ¼å¼è¾“å‡º
                formatted = f"ã€æœç´¢ç»“æœã€‘æŸ¥è¯¢: {query}\n\n"
                
                # ç»“æœè®¡æ•°
                total_results = len(combined_results)
                formatted += f"å…±æ‰¾åˆ° {total_results} æ¡ç›¸å…³ç»“æœã€‚\n\n"
                
                # å±•ç¤ºæ‰€æœ‰åˆå¹¶ç»“æœ
                for i, result in enumerate(combined_results[:max_results_per_source*2]):
                    source = result.get("source", "æœªçŸ¥")
                    title = result.get("title", "")
                    content = result.get("content", "")
                    
                    # é™åˆ¶å†…å®¹é•¿åº¦
                    if len(content) > 300:
                        content = content[:297] + "..."
                    
                    formatted += f"{i+1}. [{source}] {title}\n"
                    formatted += f"{content}\n\n"
                
                # å¦‚æœè¿˜æœ‰æ›´å¤šç»“æœ
                if len(combined_results) > max_results_per_source*2:
                    formatted += f"è¿˜æœ‰ {len(combined_results) - max_results_per_source*2} æ¡ç»“æœæœªæ˜¾ç¤ºã€‚\n"
            
            # å¦‚æœæ²¡æœ‰ä»»ä½•ç»“æœ
            if not combined_results:
                formatted += "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"
                
        else:
            # ä¸ªæ€§åŒ–æ¨¡å¼ï¼šç”Ÿæˆæ˜“äºé›†æˆåˆ°å¯¹è¯ä¸­çš„ç»“æœ
            # é¦–å…ˆæå–æœ€ç›¸å…³çš„å†…å®¹
            relevant_content = ""
            
            # ä»çŸ¥è¯†åº“å’Œç½‘ç»œæœç´¢ç»“æœä¸­æå–æœ€ç›¸å…³å†…å®¹
            if combined_results:
                # æŒ‰ç›¸ä¼¼åº¦æˆ–å…¶ä»–ç›¸å…³æ€§æŒ‡æ ‡æ’åº
                sorted_results = sorted(
                    combined_results, 
                    key=lambda x: x.get("similarity", 0) if x.get("source") == "çŸ¥è¯†åº“" else 0.5,
                    reverse=True
                )
                
                # è·å–å‰3ä¸ªæœ€ç›¸å…³ç»“æœ
                top_results = sorted_results[:3]
                
                # åˆå¹¶å†…å®¹å¹¶æ¸…ç†ä»»ä½•å¯èƒ½çš„æ³¨é‡Š
                content_pieces = []
                for result in top_results:
                    content = result.get("content", "").strip()
                    
                    # æ¸…ç†æ‰€æœ‰æ³¨é‡Šæ ¼å¼çš„å†…å®¹
                    content = re.sub(r'\s*[\(ï¼ˆ].*?[æ³¨è¨»é‡Šé‡‹][:ï¼š].*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?æ³¨æ„.*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?æ•´åˆ.*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?è½¬åŒ–.*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?æ‘˜è‡ª.*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?ç»“åˆ.*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?çŸ¥è¯†åº“.*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?æ¥æº.*?[\)ï¼‰]', '', content)
                    content = re.sub(r'\s*[\(ï¼ˆ].*?ä¿¡æ¯.*?[\)ï¼‰]', '', content)
                    # å»é™¤å†—ä½™çš„ç©ºç™½å’Œæ–­è¡Œ
                    content = re.sub(r'\n{3,}', '\n\n', content)
                    content = re.sub(r'\s+$', '', content)
                    
                    if content:
                        # å¯¹ä¸“ä¸šæ€§å†…å®¹è¿›è¡Œè½¬æ¢ï¼Œä½¿å…¶æ›´åŠ è‡ªç„¶å’Œå¯¹è¯åŒ–
                        # 1. å°†è¿‡äºæ­£å¼çš„è¡¨è¿°æ›¿æ¢ä¸ºæ›´åŠ æ—¥å¸¸åŒ–çš„è¡¨è¿°
                        content = re.sub(r'æ®æŠ¥é“', 'å¥½åƒ', content)
                        content = re.sub(r'æ ¹æ®.*?è°ƒæŸ¥', 'æœ€è¿‘', content)
                        content = re.sub(r'ç ”ç©¶è¡¨æ˜', 'å¥½åƒ', content)
                        content = re.sub(r'ä¸“å®¶è®¤ä¸º', 'æœ‰äººè¯´', content)
                        
                        # 2. åˆ é™¤è¿‡å¤šçš„è¯¦ç»†æ•°æ®ï¼Œä¿ç•™æ ¸å¿ƒä¿¡æ¯
                        content = re.sub(r'\d+\.\d+%', 'ä¸å°‘', content)
                        
                        # 3. å°†é•¿å¥å­æ‹†åˆ†æˆæ›´çŸ­çš„å¥å­
                        if len(content) > 100:
                            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', content)
                            content = '. '.join([s for s in sentences if s.strip()][:2])
                            
                        content_pieces.append(content)
                
                # åˆå¹¶åˆ°æœ€ç»ˆç»“æœ
                if content_pieces:
                    # æ„å»ºæ›´è‡ªç„¶çš„å›å¤
                    if len(content_pieces) == 1:
                        relevant_content = content_pieces[0]
                    else:
                        # ä½¿ç”¨æ›´è‡ªç„¶çš„è¿æ¥è¯
                        connectors = ["", "å¦å¤–ï¼Œ", "è¿˜æœ‰ï¼Œ", "æˆ‘è¿˜äº†è§£åˆ°ï¼Œ", "é¡ºä¾¿ä¸€æï¼Œ"]
                        connected_content = ""
                        
                        for i, piece in enumerate(content_pieces):
                            if i == 0:
                                connected_content = piece
                            else:
                                connected_content += f" {connectors[min(i, len(connectors)-1)]}{piece}"
                        
                        relevant_content = connected_content
                    
                    # é™åˆ¶æ€»é•¿åº¦å¹¶ç¡®ä¿ç»“å°¾å®Œæ•´
                    if len(relevant_content) > 150:
                        # æ‰¾åˆ°æœ€åä¸€ä¸ªå¥å·çš„ä½ç½®
                        last_period = max(relevant_content[:150].rfind('ã€‚'), 
                                         relevant_content[:150].rfind('.'), 
                                         relevant_content[:150].rfind('!'), 
                                         relevant_content[:150].rfind('ï¼'))
                        
                        if last_period > 50:  # ç¡®ä¿è‡³å°‘æœ‰è¶³å¤Ÿçš„å†…å®¹
                            relevant_content = relevant_content[:last_period+1]
                        else:
                            relevant_content = relevant_content[:147] + "..."
            
            # æ·»åŠ äººæ€§åŒ–çš„è¡¨è¾¾
            if not relevant_content:
                # æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹çš„æƒ…å†µ
                no_result_responses = [
                    "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°å…³äºè¿™ä¸ªé—®é¢˜çš„ç›¸å…³ä¿¡æ¯å‘¢ã€‚",
                    "å¯¹ä¸èµ·ï¼Œæˆ‘æ‰¾ä¸åˆ°è¿™ä¸ªé—®é¢˜çš„ç­”æ¡ˆï¼Œè¦ä¸æ¢ä¸ªè¯é¢˜èŠèŠï¼Ÿ",
                    "å—¯...æˆ‘å¥½åƒæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯ï¼Œå¯ä»¥æ¢ä¸ªé—®é¢˜é—®æˆ‘å—ï¼Ÿ",
                    "æˆ‘æŸ¥äº†ä¸€ä¸‹ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„èµ„æ–™å‘¢ï¼Œè¦ä¸æˆ‘ä»¬èŠç‚¹åˆ«çš„ï¼Ÿ"
                ]
                import random
                formatted = random.choice(no_result_responses)
            else:
                # æ‰¾åˆ°ç›¸å…³å†…å®¹çš„æƒ…å†µ
                if personalized_format_enabled:
                    # å¦‚æœæ˜¯ä¿¡æ¯æ€§å›å¤ï¼ŒåŠ å…¥é€‚å½“çš„å¼•å¯¼è¯
                    if any(keyword in query for keyword in ["ä»€ä¹ˆ", "å¦‚ä½•", "æ€ä¹ˆ", "ä¸ºä»€ä¹ˆ", "å¤šå°‘", "å“ªé‡Œ", "æ˜¯è°"]):
                        starters = [
                            "",
                            "æˆ‘äº†è§£åˆ°ï¼Œ",
                            "å…³äºè¿™ä¸ªé—®é¢˜ï¼Œ",
                            "æ ¹æ®æˆ‘æ‰€çŸ¥ï¼Œ",
                            "æˆ‘æ‰¾åˆ°çš„ä¿¡æ¯æ˜¯ï¼Œ"
                        ]
                        import random
                        formatted = f"{random.choice(starters)}{relevant_content}"
                    elif any(keyword in query for keyword in ["æœ€æ–°", "æ–°é—»", "æœ€è¿‘", "è¿›å±•", "æ¶ˆæ¯"]):
                        # å¯¹äºæ–°é—»ç±»æŸ¥è¯¢ï¼Œä½¿ç”¨æ›´æ–°é—»åŒ–çš„è¯­æ°”
                        starters = [
                            "",
                            "æœ€è¿‘ï¼Œ",
                            "æˆ‘çœ‹åˆ°ï¼Œ",
                            "æœ€æ–°æ¶ˆæ¯æ˜¯ï¼Œ",
                            "å¬è¯´"
                        ]
                        import random
                        formatted = f"{random.choice(starters)}{relevant_content}"
                    else:
                        # å¯¹äºä¸€èˆ¬é—®é¢˜ï¼Œç›´æ¥ç»™å‡ºå†…å®¹
                        formatted = relevant_content
                        
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ›´å¤šç»“æœæœªæ˜¾ç¤ºï¼Œä½¿ç”¨æ›´è‡ªç„¶çš„è¡¨è¿°
                    if len(combined_results) > 3:
                        # æ ¹æ®æƒ…å¢ƒé€‰æ‹©ä¸åŒçš„ç»“å°¾
                        if "?" in query or "ï¼Ÿ" in query:
                            # é—®é¢˜ç±»æŸ¥è¯¢
                            formatted += "\n\nè¿™äº›æ˜¯æˆ‘æ‰¾åˆ°çš„ä¸»è¦ä¿¡æ¯ï¼Œå¸Œæœ›èƒ½å¸®åˆ°ä½ ~"
                        elif any(keyword in query for keyword in ["æœ€æ–°", "æ–°é—»", "æœ€è¿‘"]):
                            # æ–°é—»ç±»æŸ¥è¯¢
                            formatted += "\n\nè¿™æ˜¯æˆ‘äº†è§£åˆ°çš„æœ€æ–°æƒ…å†µ~"
                        else:
                            # ä¸€èˆ¬æŸ¥è¯¢
                            formatted += "\n\nä»¥ä¸Šæ˜¯ç›¸å…³ä¿¡æ¯ï¼Œå¦‚æœä½ æƒ³äº†è§£æ›´å¤šç»†èŠ‚ï¼Œå¯ä»¥å†é—®æˆ‘å“¦~"
                else:
                    # ä¸å¯ç”¨æ‹ŸäººåŒ–æ ¼å¼ï¼Œåªè¿”å›å†…å®¹
                    formatted = relevant_content
        
        return combined_results, formatted
    
    def _check_cache(self, cache_key: str) -> Dict:
        """æ£€æŸ¥ç¼“å­˜
        
        Args:
            cache_key: ç¼“å­˜é”®
            
        Returns:
            Dict: ç¼“å­˜ç»“æœï¼Œæ²¡æœ‰åˆ™è¿”å›None
        """
        current_time = asyncio.get_event_loop().time()
        if cache_key in self._cache:
            cache_time, cache_data = self._cache[cache_key]
            # æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            if current_time - cache_time < self.cache_ttl:
                return cache_data
        return None
    
    def _cache_results(self, cache_key: str, content: str, results: List[Dict], source: str):
        """ç¼“å­˜æœç´¢ç»“æœ
        
        Args:
            cache_key: ç¼“å­˜é”®
            content: æ ¼å¼åŒ–çš„å†…å®¹
            results: ç»“æœåˆ—è¡¨
            source: ç»“æœæ¥æº
        """
        current_time = asyncio.get_event_loop().time()
        self._cache[cache_key] = (current_time, {
            "content": content,
            "results": results,
            "source": source
        })
    
    def _clean_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = asyncio.get_event_loop().time()
        expired_keys = []
        
        for key, (cache_time, _) in self._cache.items():
            if current_time - cache_time > self.cache_ttl:
                expired_keys.append(key)
        
        for key in expired_keys:
            del self._cache[key]
    
    def _check_cooldown(self, chat_id: str) -> Tuple[bool, float]:
        """æ£€æŸ¥æœç´¢å†·å´æ—¶é—´
        
        Args:
            chat_id: èŠå¤©ID
            
        Returns:
            Tuple[bool, float]: (æ˜¯å¦å¯ä»¥æœç´¢, å‰©ä½™å†·å´æ—¶é—´)
        """
        current_time = asyncio.get_event_loop().time()
        cooldown_seconds = float(os.getenv('SEARCH_COOLDOWN_SECONDS', str(self.default_cooldown)))
        
        if chat_id in self._last_search_times:
            last_time = self._last_search_times[chat_id]
            elapsed = current_time - last_time
            
            if elapsed < cooldown_seconds:
                # è¿˜åœ¨å†·å´ä¸­
                remaining = cooldown_seconds - elapsed
                return False, remaining
        
        # å·²ç»å†·å´å®Œæˆæˆ–æ²¡æœ‰è®°å½•
        return True, 0.0


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    async def test_search_engine():
        """æµ‹è¯•æ™ºèƒ½æœç´¢å¼•æ“å·¥å…·"""
        try:
            print("å¼€å§‹æµ‹è¯•æ™ºèƒ½æœç´¢å¼•æ“å·¥å…·...")
            search_engine = SearchEngineTool()
            
            # æµ‹è¯•æŸ¥è¯¢
            test_query = "äººå·¥æ™ºèƒ½æœ€æ–°å‘å±•"
            print(f"æµ‹è¯•æŸ¥è¯¢: {test_query}")
            
            # æ‰§è¡Œæœç´¢
            result = await search_engine.execute({
                "query": test_query,
                "chat_id": "test_chat",
                "force_web_search": True
            })
            
            # æ‰“å°ç»“æœ
            print("\næœç´¢ç»“æœ:")
            if result and "content" in result:
                print(result["content"])
            else:
                print("æœç´¢å¤±è´¥æˆ–æ— ç»“æœ")
                print(result)
            
            print("\næµ‹è¯•å®Œæˆ")
            
        except Exception as e:
            print(f"æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_search_engine()) 