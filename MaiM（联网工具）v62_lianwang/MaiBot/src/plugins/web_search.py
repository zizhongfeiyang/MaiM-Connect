import aiohttp
from bs4 import BeautifulSoup
import re
from typing import List, Dict, Optional, Union
from urllib.parse import urljoin
import os
from dotenv import load_dotenv
from src.common.logger import get_module_logger
import asyncio
import json
import socket
from src.common.utils import log_async_performance, PerformanceTimer

logger = get_module_logger("web_search")

class WebSearcher:
    def __init__(self, searxng_url: str = None, num_results: int = None):
        """
        åˆå§‹åŒ–ç½‘ç»œæœç´¢å™¨
        :param searxng_url: SearXNGå®ä¾‹çš„URLï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è·å–
        :param num_results: è¿”å›çš„ç»“æœæ•°é‡ï¼Œé»˜è®¤ä»ç¯å¢ƒå˜é‡è·å–
        """
        # æ¯æ¬¡åˆå§‹åŒ–æ—¶é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡ï¼Œç¡®ä¿è·å–æœ€æ–°é…ç½®
        load_dotenv(override=True)
        
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        self.searxng_url = searxng_url or os.getenv("SEARXNG_URL", "http://localhost:32769")
        self.searxng_url = self.searxng_url.rstrip('/')  # ç§»é™¤æœ«å°¾çš„æ–œæ 
        self.num_results = num_results or int(os.getenv("SEARXNG_RESULTS_COUNT", "10"))
        self.timeout = int(os.getenv("SEARXNG_TIMEOUT", "5"))  # ç¼©çŸ­é»˜è®¤è¶…æ—¶æ—¶é—´ä»10ç§’åˆ°5ç§’
        self.max_retries = int(os.getenv("SEARXNG_MAX_RETRIES", "2"))  # å‡å°‘é‡è¯•æ¬¡æ•°
        self.engines = os.getenv("SEARXNG_ENGINES", "google,bing,duckduckgo")
        self.search_endpoint = f"{self.searxng_url}/search"
        self.max_url_fetch = 3  # æœ€å¤šè·å–å‰3ä¸ªç»“æœçš„è¯¦ç»†å†…å®¹
        
        # è®¾ç½®é»˜è®¤è¯·æ±‚å¤´
        self.headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
            "Origin": self.searxng_url,
            "Referer": self.searxng_url + "/"
        }
        
        # æ·»åŠ è®¤è¯ä»¤ç‰Œï¼ˆå¦‚æœæœ‰ï¼‰
        auth_token = os.getenv("SEARXNG_AUTH_TOKEN")
        if auth_token and auth_token.strip():
            self.headers["Authorization"] = auth_token
        
        logger.info(f"åˆå§‹åŒ–ç½‘ç»œæœç´¢å™¨ï¼Œæœç´¢å¼•æ“URL: {self.searxng_url}")
        
        # å°è¯•æ£€æŸ¥SearXNGæœåŠ¡æ˜¯å¦å¯è®¿é—®
        self._check_service_availability()

    def _check_service_availability(self):
        """æ£€æŸ¥SearXNGæœåŠ¡æ˜¯å¦å¯è®¿é—®"""
        try:
            # ä»URLä¸­æå–ä¸»æœºå’Œç«¯å£
            url_parts = self.searxng_url.split('://')
            if len(url_parts) < 2:
                logger.warning(f"URLæ ¼å¼ä¸æ­£ç¡®: {self.searxng_url}")
                return
                
            host_port = url_parts[1].split('/')
            host_parts = host_port[0].split(':')
            
            host = host_parts[0]
            port = int(host_parts[1]) if len(host_parts) > 1 else 80
            
            logger.debug(f"æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§: {host}:{port}")
            
            # åˆ›å»ºsocketè¿æ¥æµ‹è¯•
            s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
            s.settimeout(2)  # è®¾ç½®è¶…æ—¶æ—¶é—´ä¸º2ç§’
            
            result = s.connect_ex((host, port))
            s.close()
            
            if result == 0:
                logger.info(f"SearXNGæœåŠ¡ç«¯å£å¯è®¿é—®: {host}:{port}")
            else:
                logger.warning(f"SearXNGæœåŠ¡ç«¯å£ä¸å¯è®¿é—®: {host}:{port}ï¼Œé”™è¯¯ä»£ç : {result}")
                
        except Exception as e:
            logger.warning(f"æ£€æŸ¥æœåŠ¡å¯ç”¨æ€§æ—¶å‡ºé”™: {str(e)}")

    async def test_connection(self) -> bool:
        """
        æµ‹è¯•åˆ°SearXNGæœåŠ¡çš„è¿æ¥
        :return: è¿æ¥æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"æµ‹è¯•è¿æ¥åˆ° {self.searxng_url}")
            
            # å°è¯•è¿æ¥åˆ°SearXNGé¦–é¡µ
            async with aiohttp.ClientSession() as session:
                timeout = aiohttp.ClientTimeout(total=5)  # 5ç§’è¶…æ—¶
                async with session.get(
                    self.searxng_url,
                    headers=self.headers,
                    timeout=timeout
                ) as response:
                    if response.status == 200:
                        logger.info(f"æˆåŠŸè¿æ¥åˆ°SearXNGæœåŠ¡ï¼ŒçŠ¶æ€ç : {response.status}")
                        html = await response.text()
                        logger.debug(f"é¦–é¡µå“åº”é•¿åº¦: {len(html)}")
                        
                        # æ£€æŸ¥æ˜¯å¦å«æœ‰æœç´¢è¡¨å•
                        soup = BeautifulSoup(html, 'html.parser')
                        search_form = soup.find('form')
                        if search_form:
                            logger.debug("æ‰¾åˆ°æœç´¢è¡¨å•ï¼ŒæœåŠ¡æ­£å¸¸")
                            return True
                        else:
                            logger.warning("æœªæ‰¾åˆ°æœç´¢è¡¨å•ï¼Œå“åº”å¯èƒ½ä¸æ˜¯SearXNGé¡µé¢")
                            return False
                    else:
                        logger.warning(f"è¿æ¥åˆ°SearXNGæœåŠ¡å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}")
                        return False
        except Exception as e:
            logger.error(f"æµ‹è¯•è¿æ¥å¤±è´¥: {str(e)}")
            return False

    @log_async_performance
    async def search_web(self, query: str, time_range: str = "month") -> List[Dict]:
        """
        ä½¿ç”¨SearXNGè¿›è¡Œç½‘ç»œæœç´¢
        :param query: æœç´¢æŸ¥è¯¢
        :param time_range: æœç´¢æ—¶é—´èŒƒå›´ï¼Œå¯é€‰å€¼: "day", "week", "month", "year"ï¼Œé»˜è®¤ä¸º"month"
        :return: æœç´¢ç»“æœåˆ—è¡¨
        """
        if not query or not query.strip():
            logger.error("æœç´¢æŸ¥è¯¢ä¸èƒ½ä¸ºç©º")
            return []
        
        # æ·»åŠ æ€»ä½“è¶…æ—¶æ§åˆ¶
        try:
            # è®¾ç½®15ç§’æ€»è¶…æ—¶ï¼Œç¡®ä¿å³ä½¿å‡ºé—®é¢˜ä¹Ÿèƒ½æ­£å¸¸è¿”å›
            return await asyncio.wait_for(
                self._search_web_impl(query, time_range), 
                timeout=15.0
            )
        except asyncio.TimeoutError:
            logger.warning(f"æœç´¢æ€»ä½“è¶…æ—¶: {query}")
            return []  # è¿”å›ç©ºç»“æœä»¥é¿å…æ— é™ç­‰å¾…
            
    async def _search_web_impl(self, query: str, time_range: str = "month") -> List[Dict]:
        """å®é™…æ‰§è¡Œæœç´¢çš„å®ç°æ–¹æ³•"""
        with PerformanceTimer(f"search_web-{query[:20]}") as timer:
            # å…ˆæµ‹è¯•è¿æ¥
            timer.start_section("test_connection")
            connection_ok = await self.test_connection()
            timer.end_section()
            
            if not connection_ok:
                logger.error("SearXNGæœåŠ¡è¿æ¥æµ‹è¯•å¤±è´¥ï¼Œæ— æ³•æ‰§è¡Œæœç´¢")
                return []
                
            try:
                query = query.strip()
                logger.info(f"æ‰§è¡Œç½‘ç»œæœç´¢: {query}ï¼Œæ—¶é—´èŒƒå›´: {time_range}")
                
                # éªŒè¯æ—¶é—´èŒƒå›´å‚æ•°
                valid_time_ranges = ["day", "week", "month", "year", ""]
                if time_range not in valid_time_ranges:
                    logger.warning(f"æ— æ•ˆçš„æ—¶é—´èŒƒå›´: {time_range}ï¼Œä½¿ç”¨é»˜è®¤å€¼'month'")
                    time_range = "month"
                
                # ä½¿ç”¨å®ä¾‹çš„engineså±æ€§
                logger.debug(f"ä½¿ç”¨æœç´¢å¼•æ“: {self.engines}")
                
                # æ„å»ºæœç´¢å‚æ•° - å°è¯•ä¸¤ç§æ ¼å¼ï¼šHTMLå’ŒJSON
                formats = ["html", "json"]
                html = None
                results_data = None
                
                # å°è¯•ä¸åŒçš„æ ¼å¼
                timer.start_section("search_request")
                for fmt in formats:
                    try:
                        # æ„å»ºæœç´¢å‚æ•°
                        data = {
                            "q": query,
                            "category_general": "1",
                            "time_range": time_range,
                            "language": "zh-CN",
                            "engines": self.engines,
                            "format": fmt
                        }
                        
                        logger.debug(f"å°è¯•ä½¿ç”¨ {fmt} æ ¼å¼æœç´¢")
                        
                        # ä½¿ç”¨é‡è¯•æœºåˆ¶å‘é€è¯·æ±‚
                        for retry in range(self.max_retries):
                            try:
                                async with aiohttp.ClientSession() as session:
                                    logger.debug(f"å‘é€GETè¯·æ±‚åˆ° {self.search_endpoint}ï¼Œå‚æ•°: {data}")
                                    timeout = aiohttp.ClientTimeout(total=self.timeout)
                                    async with session.get(
                                        self.search_endpoint,
                                        params=data,
                                        headers=self.headers,
                                        timeout=timeout
                                    ) as response:
                                        response.raise_for_status()
                                        logger.debug(f"æ”¶åˆ°å“åº”çŠ¶æ€ç : {response.status}")
                                        
                                        if fmt == "json":
                                            try:
                                                results_data = await response.json()
                                                logger.debug(f"æˆåŠŸè·å–JSONå“åº”: {str(results_data)[:200]}...")
                                                break  # æˆåŠŸè·å–JSONæ•°æ®
                                            except json.JSONDecodeError:
                                                logger.warning("JSONè§£æå¤±è´¥ï¼Œç»§ç»­å°è¯•å…¶ä»–æ ¼å¼")
                                                results_data = None
                                                # ç»§ç»­å°è¯•å…¶ä»–æ ¼å¼
                                                break
                                        else:  # html
                                            html = await response.text()
                                            logger.debug(f"æ”¶åˆ°å“åº”HTMLé•¿åº¦: {len(html)}")
                                            # æ‰“å°HTMLçš„å¼€å¤´éƒ¨åˆ†ï¼Œå¸®åŠ©è¯Šæ–­
                                            if len(html) > 0:
                                                logger.debug(f"HTMLå“åº”å¼€å¤´: {html[:200]}...")
                                            break  # è·å–åˆ°HTMLå“åº”
                            except Exception as e:
                                if retry == self.max_retries - 1:  # æœ€åä¸€æ¬¡é‡è¯•
                                    logger.warning(f"{fmt}æ ¼å¼è¯·æ±‚å¤±è´¥: {e}")
                                    break  # å°è¯•ä¸‹ä¸€ç§æ ¼å¼
                                logger.warning(f"æœç´¢è¯·æ±‚å¤±è´¥ï¼Œæ­£åœ¨é‡è¯• ({retry + 1}/{self.max_retries}): {e}")
                                await asyncio.sleep(2 ** retry)  # æŒ‡æ•°é€€é¿
                    
                        # æ£€æŸ¥æ˜¯å¦æˆåŠŸè·å–åˆ°å“åº”
                        if fmt == "json" and results_data:
                            # å¤„ç†JSONå“åº”
                            timer.end_section()
                            timer.start_section("process_json_results")
                            result = self._process_json_results(results_data)
                            timer.end_section()
                            return result
                        elif fmt == "html" and html:
                            # å·²è·å–HTMLå“åº”ï¼Œé€€å‡ºå¾ªç¯
                            break
                            
                    except Exception as e:
                        logger.warning(f"{fmt}æ ¼å¼å¤„ç†å¤±è´¥: {e}")
                        continue  # å°è¯•ä¸‹ä¸€ç§æ ¼å¼
                timer.end_section()
                
                # å¦‚æœæ— æ³•è·å–ä»»ä½•æ ¼å¼çš„æœ‰æ•ˆå“åº”ï¼Œè¿”å›ç©ºåˆ—è¡¨
                if not html and not results_data:
                    logger.error("æ‰€æœ‰æ ¼å¼çš„æœç´¢è¯·æ±‚å‡å¤±è´¥")
                    return []
                
                # å¤„ç†HTMLå“åº”ï¼ˆå¦‚æœJSONå¤„ç†å¤±è´¥ï¼‰
                if html:
                    timer.start_section("process_html_results")
                    # è§£æHTMLå“åº”
                    soup = BeautifulSoup(html, 'html.parser')
                    results = []
                    
                    # æŸ¥æ‰¾æ‰€æœ‰æœç´¢ç»“æœ
                    result_articles = soup.select('html > body > main > div > div:nth-child(2) > article')
                    logger.debug(f"CSSé€‰æ‹©å™¨1æ‰¾åˆ° {len(result_articles)} ä¸ªç»“æœ")
                    
                    if not result_articles:
                        # å°è¯•å…¶ä»–å¯èƒ½çš„CSSé€‰æ‹©å™¨
                        result_articles = soup.select('article.result')
                        logger.debug(f"CSSé€‰æ‹©å™¨2æ‰¾åˆ° {len(result_articles)} ä¸ªç»“æœ")
                        
                    if not result_articles:
                        # å†æ¬¡å°è¯•å…¶ä»–é€‰æ‹©å™¨
                        result_articles = soup.select('.result')
                        logger.debug(f"CSSé€‰æ‹©å™¨3æ‰¾åˆ° {len(result_articles)} ä¸ªç»“æœ")
                    
                    if not result_articles:
                        # å°è¯•æ›´é€šç”¨çš„é€‰æ‹©å™¨
                        result_articles = soup.select('article')
                        logger.debug(f"CSSé€‰æ‹©å™¨4 (article) æ‰¾åˆ° {len(result_articles)} ä¸ªç»“æœ")
                        
                        # å¦‚æœä»ç„¶æ‰¾ä¸åˆ°ç»“æœï¼Œå°è¯•åˆ†æé¡µé¢ç»“æ„
                        if not result_articles:
                            logger.debug("æ— æ³•æ‰¾åˆ°æœç´¢ç»“æœï¼Œåˆ†æé¡µé¢ç»“æ„...")
                            main_tags = soup.find_all('main')
                            logger.debug(f"æ‰¾åˆ° {len(main_tags)} ä¸ª main æ ‡ç­¾")
                            
                            div_tags = soup.find_all('div')
                            logger.debug(f"æ‰¾åˆ° {len(div_tags)} ä¸ª div æ ‡ç­¾")
                            
                            # å°è¯•æ‰¾å‡ºé¡µé¢ç»“æ„é—®é¢˜
                            form_tags = soup.find_all('form')
                            logger.debug(f"æ‰¾åˆ° {len(form_tags)} ä¸ª form æ ‡ç­¾ï¼Œå¯èƒ½æ˜¯æœç´¢é¡µè€Œéç»“æœé¡µ")
                    
                    logger.info(f"æ‰¾åˆ° {len(result_articles)} ä¸ªæœç´¢ç»“æœ")
                    
                    # å¦‚æœæ‰¾åˆ°ç»“æœï¼Œè®°å½•ç¬¬ä¸€ä¸ªç»“æœçš„ç»“æ„ï¼Œå¸®åŠ©è°ƒè¯•
                    if result_articles and len(result_articles) > 0:
                        logger.debug(f"ç¬¬ä¸€ä¸ªç»“æœHTML: {result_articles[0]}")
                    
                    timer.start_section("process_results")
                    
                    # å¤„ç†å‰max_url_fetchä¸ªç»“æœçš„åŸºæœ¬ä¿¡æ¯
                    limited_results = result_articles[:min(self.num_results, len(result_articles))]
                    basic_results = []
                    urls_to_fetch = []
                    
                    # å…ˆæå–åŸºæœ¬ä¿¡æ¯
                    for article in limited_results:
                        try:
                            # æå–æ ‡é¢˜
                            title_elem = article.find('h3')
                            title = title_elem.get_text().strip() if title_elem else "æ— æ ‡é¢˜"
                            
                            # æå–é“¾æ¥
                            url_elem = article.find('a')
                            url = url_elem['href'] if url_elem else ""
                            
                            # æå–æ—¥æœŸï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            date_elem = article.select_one('.published')
                            published_date = date_elem.get_text().strip() if date_elem else ""
                            
                            # æå–å†…å®¹
                            content_elem = article.find('p')
                            content = content_elem.get_text().strip() if content_elem else ""
                            
                            logger.debug(f"è§£æç»“æœ: æ ‡é¢˜='{title}', URL='{url}', æ—¥æœŸ='{published_date}', å†…å®¹é•¿åº¦={len(content)}")
                            
                            result_item = {
                                "title": title,
                                "url": url,
                                "content": content,
                                "published_date": published_date
                            }
                            
                            basic_results.append(result_item)
                            
                            # ä»…æ”¶é›†å‰max_url_fetchä¸ªæœ‰æ•ˆURLè¿›è¡Œå†…å®¹è·å–
                            if url and len(urls_to_fetch) < self.max_url_fetch:
                                urls_to_fetch.append((len(basic_results)-1, url))
                                
                        except Exception as e:
                            logger.error(f"å¤„ç†æœç´¢ç»“æœå¤±è´¥: {str(e)}")
                            continue
                    
                    # å¹¶è¡Œè·å–URLå†…å®¹
                    timer.start_section("fetch_url_contents")
                    if urls_to_fetch:
                        tasks = []
                        for idx, url in urls_to_fetch:
                            tasks.append(self._fetch_and_update_content(idx, url, basic_results))
                        
                        # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è·å–ä»»åŠ¡
                        await asyncio.gather(*tasks)
                    timer.end_section()
                    
                    timer.end_section()  # end process_results
                    timer.end_section()  # end process_html_results
                    
                    return basic_results
                
                # å¦‚æœæ‰€æœ‰å¤„ç†éƒ½å¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
                return []
                
            except Exception as e:
                logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
                return []
    
    async def _fetch_and_update_content(self, idx: int, url: str, results: List[Dict]) -> None:
        """å¹¶è¡Œè·å–URLå†…å®¹å¹¶æ›´æ–°ç»“æœåˆ—è¡¨
        
        Args:
            idx: ç»“æœåˆ—è¡¨ä¸­çš„ç´¢å¼•
            url: è¦è·å–å†…å®¹çš„URL
            results: è¦æ›´æ–°çš„ç»“æœåˆ—è¡¨
        """
        try:
            content = await self.get_url_content(url)
            if content and idx < len(results):
                # æ›´æ–°å¯¹åº”ç»“æœçš„å†…å®¹
                original_content = results[idx]['content']
                if content:
                    results[idx]['content'] = f"{original_content}\n\nè¯¦ç»†å†…å®¹ï¼š\n{content}"
        except Exception as e:
            logger.error(f"è·å–URLå†…å®¹å¤±è´¥: {url}, é”™è¯¯: {str(e)}")

    async def get_url_content(self, url: str) -> str:
        """
        è·å–URLçš„å†…å®¹
        :param url: è¦è®¿é—®çš„URL
        :return: é¡µé¢å†…å®¹çš„æ–‡æœ¬
        """
        if not url or not url.startswith(('http://', 'https://')):
            logger.warning(f"æ— æ•ˆURL: {url}")
            return ""
            
        try:
            # åˆ›å»ºè‡ªå®šä¹‰çš„è¶…æ—¶è®¾ç½® - å‡å°‘è¶…æ—¶æ—¶é—´
            timeout = aiohttp.ClientTimeout(total=self.timeout)
            async with aiohttp.ClientSession(timeout=timeout) as session:
                # æ·»åŠ è¶…æ—¶å¤„ç†å’Œé”™è¯¯é‡è¯•
                for retry in range(1):  # å‡å°‘é‡è¯•æ¬¡æ•°åˆ°1æ¬¡
                    try:
                        async with session.get(url, headers=self.headers) as response:
                            response.raise_for_status()
                            html = await response.text()
                            break
                    except asyncio.TimeoutError:
                        logger.warning(f"è·å–URLå†…å®¹è¶…æ—¶: {url}")
                        return ""
                    except Exception as e:
                        logger.debug(f"è·å–URLå†…å®¹å¤±è´¥: {url}, é”™è¯¯: {e}")
                        return ""
            
            # è§£æHTML
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç§»é™¤è„šæœ¬å’Œæ ·å¼
            for script in soup(["script", "style"]):
                script.decompose()
            
            # è·å–æ–‡æœ¬
            text = soup.get_text()
            
            # æ¸…ç†æ–‡æœ¬
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            return text[:1000]  # é™åˆ¶è¿”å›çš„æ–‡æœ¬é•¿åº¦ï¼Œä»2000ç¼©çŸ­åˆ°1000å­—ç¬¦
        except Exception as e:
            logger.error(f"è·å–URLå†…å®¹å¤±è´¥: {str(e)}, URL: {url}")
            return ""

    def _process_json_results(self, json_data: Dict) -> List[Dict]:
        """å¤„ç†JSONæ ¼å¼çš„æœç´¢ç»“æœ
        
        :param json_data: SearXNGè¿”å›çš„JSONæ•°æ®
        :return: å¤„ç†åçš„ç»“æœåˆ—è¡¨
        """
        results = []
        try:
            if not isinstance(json_data, dict):
                logger.warning(f"JSONæ•°æ®æ ¼å¼ä¸æ­£ç¡®: {type(json_data)}")
                return []
                
            # è·å–ç»“æœåˆ—è¡¨
            result_items = json_data.get("results", [])
            if not result_items:
                logger.warning("JSONç»“æœä¸ºç©º")
                return []
                
            logger.info(f"ä»JSONä¸­æ‰¾åˆ° {len(result_items)} ä¸ªæœç´¢ç»“æœ")
            
            # å¤„ç†æ¯ä¸ªç»“æœ
            for item in result_items[:self.num_results]:
                try:
                    title = item.get("title", "æ— æ ‡é¢˜")
                    url = item.get("url", "")
                    content = item.get("content", "")
                    published_date = item.get("publishedDate", "")
                    
                    # å¤„ç†ç»“æœ
                    results.append({
                        "title": title,
                        "url": url,
                        "content": content,
                        "published_date": published_date
                    })
                except Exception as e:
                    logger.error(f"å¤„ç†JSONæœç´¢ç»“æœé¡¹å¤±è´¥: {e}")
                    continue
                
            return results
        except Exception as e:
            logger.error(f"å¤„ç†JSONæœç´¢ç»“æœå¤±è´¥: {e}")
            return []

    def format_results(self, results: List[Dict]) -> str:
        """
        æ ¼å¼åŒ–æœç´¢ç»“æœ
        :param results: æœç´¢ç»“æœåˆ—è¡¨
        :return: æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        if not results:
            return "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœã€‚"
        
        formatted = "ğŸ” æœç´¢ç»“æœï¼š\n\n"
        for i, result in enumerate(results, 1):
            formatted += f"ğŸ“Œ ç»“æœ {i}:\n"
            formatted += f"   ğŸ“ æ ‡é¢˜: {result['title']}\n"
            formatted += f"   ğŸ”— é“¾æ¥: {result['url']}\n"
            if result.get('published_date'):
                formatted += f"   ğŸ“… å‘å¸ƒæ—¥æœŸ: {result['published_date']}\n"
            
            # å¤„ç†å†…å®¹ï¼Œç¡®ä¿æ ¼å¼æ¸…æ™°
            content = result['content'].strip()
            if len(content) > 500:
                content = content[:500] + "..."
            
            # æŒ‰æ®µè½åˆ†å‰²å†…å®¹
            paragraphs = [p.strip() for p in content.split('\n') if p.strip()]
            formatted += "   ğŸ“„ å†…å®¹:\n"
            for para in paragraphs:
                formatted += f"      {para}\n"
            
            formatted += "\n"  # ç»“æœä¹‹é—´çš„åˆ†éš”ç©ºè¡Œ
        
        return formatted

    def update_config(self):
        """
        æ›´æ–°æœç´¢å™¨é…ç½®ï¼Œé‡æ–°ä»ç¯å¢ƒå˜é‡è¯»å–
        ç”¨äºåœ¨è¿è¡Œæ—¶æ›´æ–°é…ç½®è€Œä¸éœ€è¦é‡æ–°åˆ›å»ºå®ä¾‹
        """
        # é‡æ–°åŠ è½½ç¯å¢ƒå˜é‡
        load_dotenv(override=True)
        
        # æ›´æ–°é…ç½®
        self.searxng_url = os.getenv("SEARXNG_URL", "http://localhost:32769")
        self.searxng_url = self.searxng_url.rstrip('/')  # ç§»é™¤æœ«å°¾çš„æ–œæ 
        self.num_results = int(os.getenv("SEARXNG_RESULTS_COUNT", "10"))
        self.timeout = int(os.getenv("SEARXNG_TIMEOUT", "5"))  # ç¼©çŸ­é»˜è®¤è¶…æ—¶æ—¶é—´ä»10ç§’åˆ°5ç§’
        self.max_retries = int(os.getenv("SEARXNG_MAX_RETRIES", "2"))  # å‡å°‘é‡è¯•æ¬¡æ•°
        self.engines = os.getenv("SEARXNG_ENGINES", "google,bing,duckduckgo")
        self.search_endpoint = f"{self.searxng_url}/search"
        self.max_url_fetch = 3  # æœ€å¤šè·å–å‰3ä¸ªç»“æœçš„è¯¦ç»†å†…å®¹
        
        # æ›´æ–°è®¤è¯ä»¤ç‰Œ
        auth_token = os.getenv("SEARXNG_AUTH_TOKEN")
        if "Authorization" in self.headers and not (auth_token and auth_token.strip()):
            # ä¹‹å‰æœ‰ä»¤ç‰Œä½†ç°åœ¨æ²¡æœ‰ï¼Œç§»é™¤
            del self.headers["Authorization"]
        elif auth_token and auth_token.strip():
            # æ›´æ–°æˆ–æ·»åŠ ä»¤ç‰Œ
            self.headers["Authorization"] = auth_token
            
        # æ›´æ–°å¤´éƒ¨å¼•ç”¨é“¾æ¥
        self.headers["Origin"] = self.searxng_url
        self.headers["Referer"] = self.searxng_url + "/"
        
        logger.info(f"æ›´æ–°ç½‘ç»œæœç´¢å™¨é…ç½®ï¼Œæœç´¢å¼•æ“URL: {self.searxng_url}")

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # ä»ç¯å¢ƒå˜é‡è·å–æœ€æ–°é…ç½®åˆ›å»ºWebSearcherå®ä¾‹
    searcher = WebSearcher()
    query = "ç¾å›½æ–°é—»"
    
    import asyncio
    
    async def test_search():
        # å…ˆæµ‹è¯•è¿æ¥
        connection_ok = await searcher.test_connection()
        if not connection_ok:
            print("æ— æ³•è¿æ¥åˆ°SearXNGæœåŠ¡ï¼Œè¯·æ£€æŸ¥é…ç½®å’ŒæœåŠ¡çŠ¶æ€")
            return
            
        print(f"æ­£åœ¨æœç´¢: {query}")
        results = await searcher.search_web(query)
        print(searcher.format_results(results))
    
    # è¿è¡Œæµ‹è¯•
    loop = asyncio.get_event_loop()
    loop.run_until_complete(test_search()) 